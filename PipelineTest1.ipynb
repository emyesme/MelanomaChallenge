{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10c2bdab",
   "metadata": {},
   "source": [
    "### GENERATING FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6d0f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46964cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import library\n",
    "import color_features\n",
    "import texture_features\n",
    "import glcm_features\n",
    "import hair_removal\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "samples = library.get_sample(path = \"/home/name/Desktop/CAD/challenge1/train\", amount=0)\n",
    "\n",
    "dictF = {}\n",
    "features = pd.DataFrame()\n",
    "count = 0\n",
    "flag = True\n",
    "for sample in samples:\n",
    "    print('count ', count)\n",
    "    count += 1\n",
    "    img = cv2.imread(sample)\n",
    "    output_bh = library.hair_removal_BH(img)\n",
    "    \n",
    "    dictF['name'] = sample\n",
    "    dictF['label'] = (0 if 'nevus' in sample else 1 )\n",
    "    \n",
    "    # color features\n",
    "    colors = color_features.extract_color_features(output_bh)\n",
    "    \n",
    "    dictF.update(colors)\n",
    "    \n",
    "    #glcm features\n",
    "    angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "    distances = [1]\n",
    "    colorspaces = ['rgb', 'hsv', 'lab', 'ycc', 'gray']\n",
    "\n",
    "    for cs in colorspaces:\n",
    "        glcm = glcm_features.get_glcm(output_bh, angles, distances, cs)\n",
    "        dictF.update(glcm)\n",
    "    \n",
    "    # lbp features\n",
    "    lbp = texture_features.extract_lbp(output_bh, 1, 8)\n",
    "    dictF.update(lbp)\n",
    "    \n",
    "    # orb features\n",
    "    # orb = texture_features.extract_orb(output_bh, 64)\n",
    "    # dictF.update(orb)\n",
    "    \n",
    "    features = features.append(dictF, ignore_index=True)\n",
    "    \n",
    "    library.writeFeatures(features,\n",
    "                  flag,\n",
    "                  os.path.join('/home',\n",
    "                             'name',\n",
    "                             'Desktop',\n",
    "                             'CAD'),\n",
    "                  'features_train_bh_3000.csv')\n",
    "    \n",
    "    flag = False\n",
    "    features = pd.DataFrame()\n",
    "    dictF.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bf9e210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best hyperparameters\n",
      "The best parameters for rf are {'criterion': 'gini', 'max_depth': 10, 'n_estimators': 500} with an accuracy of 0.7367\n",
      "### RF ###\n",
      "current pipeline\n",
      "Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('selector rfe',\n",
      "                 RFE(estimator=RandomForestClassifier(n_jobs=-1,\n",
      "                                                      random_state=42))),\n",
      "                ('clf',\n",
      "                 RandomForestClassifier(max_depth=10, n_estimators=500,\n",
      "                                        n_jobs=-1, random_state=42))])\n",
      "###############\n",
      " ### Report ###\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.71      0.77      1198\n",
      "           1       0.75      0.85      0.80      1202\n",
      "\n",
      "    accuracy                           0.78      2400\n",
      "   macro avg       0.79      0.78      0.78      2400\n",
      "weighted avg       0.79      0.78      0.78      2400\n",
      "\n",
      " ### score ###\n",
      "0.78125\n",
      " ### accuracy ###\n",
      "0.78125\n",
      "### f1_score ###\n",
      "0.7953216374269007\n",
      "### confusion matrix ###\n",
      "[[ 855  343]\n",
      " [ 182 1020]]\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "### TREE ###\n",
      "current pipeline\n",
      "Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('selector rfe',\n",
      "                 RFE(estimator=RandomForestClassifier(n_jobs=-1,\n",
      "                                                      random_state=42))),\n",
      "                ('clf', DecisionTreeClassifier())])\n",
      "###############\n",
      " ### Report ###\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.68      0.69      1198\n",
      "           1       0.69      0.71      0.70      1202\n",
      "\n",
      "    accuracy                           0.70      2400\n",
      "   macro avg       0.70      0.70      0.70      2400\n",
      "weighted avg       0.70      0.70      0.70      2400\n",
      "\n",
      " ### score ###\n",
      "0.6979166666666666\n",
      " ### accuracy ###\n",
      "0.6979166666666666\n",
      "### f1_score ###\n",
      "0.7029905776321181\n",
      "### confusion matrix ###\n",
      "[[817 381]\n",
      " [344 858]]\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "The best parameters for svm are {'C': 1, 'gamma': 2.5, 'kernel': 'rbf'} with an accuracy of 0.5317\n",
      "### SVM ###\n",
      "current pipeline\n",
      "Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('selector rfe',\n",
      "                 RFE(estimator=RandomForestClassifier(n_jobs=-1,\n",
      "                                                      random_state=42))),\n",
      "                ('clf',\n",
      "                 SVC(C=1, gamma=2.5, probability=True, random_state=42))])\n",
      "###############\n",
      " ### Report ###\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.01      0.01      1198\n",
      "           1       0.50      1.00      0.67      1202\n",
      "\n",
      "    accuracy                           0.50      2400\n",
      "   macro avg       0.75      0.50      0.34      2400\n",
      "weighted avg       0.75      0.50      0.34      2400\n",
      "\n",
      " ### score ###\n",
      "0.5033333333333333\n",
      " ### accuracy ###\n",
      "0.5033333333333333\n",
      "### f1_score ###\n",
      "0.6685205784204672\n",
      "### confusion matrix ###\n",
      "[[   6 1192]\n",
      " [   0 1202]]\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "The best parameters for ab are {'learning_rate': 0.1, 'n_estimators': 600} with an accuracy of 0.7667\n",
      "### ADABOOST ###\n",
      "current pipeline\n",
      "Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('selector rfe',\n",
      "                 RFE(estimator=RandomForestClassifier(n_jobs=-1,\n",
      "                                                      random_state=42))),\n",
      "                ('clf',\n",
      "                 AdaBoostClassifier(learning_rate=0.1, n_estimators=600,\n",
      "                                    random_state=42))])\n",
      "###############\n",
      " ### Report ###\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.73      0.76      1198\n",
      "           1       0.75      0.82      0.79      1202\n",
      "\n",
      "    accuracy                           0.78      2400\n",
      "   macro avg       0.78      0.78      0.78      2400\n",
      "weighted avg       0.78      0.78      0.78      2400\n",
      "\n",
      " ### score ###\n",
      "0.7758333333333334\n",
      " ### accuracy ###\n",
      "0.7758333333333334\n",
      "### f1_score ###\n",
      "0.7861685214626392\n",
      "### confusion matrix ###\n",
      "[[873 325]\n",
      " [213 989]]\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "The best parameters for gb are {'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 300} with an accuracy of 0.7300\n",
      "### GRADBOOST ###\n",
      "current pipeline\n",
      "Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('selector rfe',\n",
      "                 RFE(estimator=RandomForestClassifier(n_jobs=-1,\n",
      "                                                      random_state=42))),\n",
      "                ('clf',\n",
      "                 GradientBoostingClassifier(max_depth=8, n_estimators=300,\n",
      "                                            random_state=42))])\n",
      "###############\n",
      " ### Report ###\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.76      0.79      1198\n",
      "           1       0.78      0.82      0.80      1202\n",
      "\n",
      "    accuracy                           0.79      2400\n",
      "   macro avg       0.80      0.79      0.79      2400\n",
      "weighted avg       0.80      0.79      0.79      2400\n",
      "\n",
      " ### score ###\n",
      "0.7945833333333333\n",
      " ### accuracy ###\n",
      "0.7945833333333333\n",
      "### f1_score ###\n",
      "0.8008080808080809\n",
      "### confusion matrix ###\n",
      "[[916 282]\n",
      " [211 991]]\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "### HISTGRADBOOST ###\n",
      "current pipeline\n",
      "Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('selector rfe',\n",
      "                 RFE(estimator=RandomForestClassifier(n_jobs=-1,\n",
      "                                                      random_state=42))),\n",
      "                ('clf', HistGradientBoostingClassifier())])\n",
      "###############\n",
      " ### Report ###\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.78      0.79      1198\n",
      "           1       0.79      0.82      0.80      1202\n",
      "\n",
      "    accuracy                           0.80      2400\n",
      "   macro avg       0.80      0.80      0.80      2400\n",
      "weighted avg       0.80      0.80      0.80      2400\n",
      "\n",
      " ### score ###\n",
      "0.7979166666666667\n",
      " ### accuracy ###\n",
      "0.7979166666666667\n",
      "### f1_score ###\n",
      "0.8021215830273357\n",
      "### confusion matrix ###\n",
      "[[932 266]\n",
      " [219 983]]\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "### KNN ###\n",
      "current pipeline\n",
      "Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('selector rfe',\n",
      "                 RFE(estimator=RandomForestClassifier(n_jobs=-1,\n",
      "                                                      random_state=42))),\n",
      "                ('clf', KNeighborsClassifier())])\n",
      "###############\n",
      " ### Report ###\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.68      0.72      1198\n",
      "           1       0.71      0.78      0.74      1202\n",
      "\n",
      "    accuracy                           0.73      2400\n",
      "   macro avg       0.73      0.73      0.73      2400\n",
      "weighted avg       0.73      0.73      0.73      2400\n",
      "\n",
      " ### score ###\n",
      "0.7316666666666667\n",
      " ### accuracy ###\n",
      "0.7316666666666667\n",
      "### f1_score ###\n",
      "0.7446471054718478\n",
      "### confusion matrix ###\n",
      "[[817 381]\n",
      " [263 939]]\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "### LDA ###\n",
      "current pipeline\n",
      "Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('selector rfe',\n",
      "                 RFE(estimator=RandomForestClassifier(n_jobs=-1,\n",
      "                                                      random_state=42))),\n",
      "                ('clf', LinearDiscriminantAnalysis())])\n",
      "###############\n",
      " ### Report ###\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.74      0.76      1198\n",
      "           1       0.75      0.80      0.78      1202\n",
      "\n",
      "    accuracy                           0.77      2400\n",
      "   macro avg       0.77      0.77      0.77      2400\n",
      "weighted avg       0.77      0.77      0.77      2400\n",
      "\n",
      " ### score ###\n",
      "0.7691666666666667\n",
      " ### accuracy ###\n",
      "0.7691666666666667\n",
      "### f1_score ###\n",
      "0.7766129032258065\n",
      "### confusion matrix ###\n",
      "[[883 315]\n",
      " [239 963]]\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import library\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2, mutual_info_classif    \n",
    "\n",
    "#\n",
    "classifiers = [\"rf\", \"tree\", \"svm\", \"adaboost\", \"gradboost\", \"histgradboost\", \"knn\", \"lda\"]\n",
    "\n",
    "train = pd.read_csv(os.path.join('/home','emily','Desktop','CAD','features_train_bh_3000.csv'))\n",
    "test = pd.read_csv(os.path.join('/home','emily','Desktop','CAD','features_test_bh_3000.csv'))\n",
    "\n",
    "y = train['label']\n",
    "X = train.drop(['label'], axis=1)\n",
    "X = X.drop(['name'], axis=1)\n",
    "\n",
    "\n",
    "y_test = test['label']\n",
    "X_test = test.drop(['label'], axis=1)\n",
    "X_test = X_test.drop(['name'], axis=1)\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.2, random_state=1)\n",
    "\n",
    "\n",
    "\n",
    "# preprocessing options\n",
    "#('selectFromModel', SelectFromModel(RandomForestClassifier(random_state=42, n_jobs = -1)))\n",
    "#('selector rfe', RFE(RandomForestClassifier(random_state=42, n_jobs = -1))),\n",
    "#('reduce_dims', PCA(n_components=150)),\n",
    "#('mutual_info_classif, SelectKBest(mutual_info_classif, k=100)),\n",
    "\n",
    "for classifier in classifiers:\n",
    "    \n",
    "    # preprocessing steps\n",
    "    pipe = [('scale', StandardScaler()),\n",
    "            ('selector rfe', RFE(RandomForestClassifier(random_state=42, n_jobs = -1)))\n",
    "           ]\n",
    "\n",
    "    \n",
    "    if classifier == \"svm\":\n",
    "        clf, best_params = library.SVC_linear(X_val, y_val, cv=2)\n",
    "        clf.set_params(**best_params)\n",
    "        print(\"### SVM ###\")\n",
    "    \n",
    "    elif classifier == \"rf\":\n",
    "        clf, best_params = library.RandomForest(X_val, y_val, cv=2)\n",
    "        clf.set_params(**best_params)\n",
    "        print(\"### RF ###\")\n",
    "    \n",
    "    elif classifier == \"tree\":\n",
    "        clf = tree.DecisionTreeClassifier()\n",
    "        print(\"### TREE ###\")\n",
    "    \n",
    "    elif classifier == \"adaboost\":\n",
    "        clf, best_params = library.AdaBoost(X_val, y_val)\n",
    "        clf.set_params(**best_params)\n",
    "        print(\"### ADABOOST ###\")\n",
    "    \n",
    "    elif classifier == \"gradboost\":\n",
    "        clf, best_params = library.GradientBoosting(X_val, y_val)\n",
    "        clf.set_params(**best_params)\n",
    "        print(\"### GRADBOOST ###\")\n",
    "    \n",
    "    elif classifier == \"knn\":\n",
    "        clf, best_params = library.knn(X_val, y_val)\n",
    "        clf.set_params(**best_params)\n",
    "        print(\"### KNN ###\")\n",
    "        \n",
    "    elif classifier == \"histgradboost\":\n",
    "        clf = HistGradientBoostingClassifier()\n",
    "        print(\"### HISTGRADBOOST ###\")\n",
    "        \n",
    "    elif classifier == \"lda\":\n",
    "        clf = LinearDiscriminantAnalysis()\n",
    "        print(\"### LDA ###\")        \n",
    "        \n",
    "    # add classifier \n",
    "    pipe.append(tuple(('clf', clf)))\n",
    "    \n",
    "    steps = Pipeline(pipe)\n",
    "    \n",
    "    # pipeline shape\n",
    "    print(\"current pipeline\")\n",
    "    print(steps)\n",
    "    \n",
    "    \n",
    "    library.fit_report(steps, X_train, y_train, X_test, y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbf241b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
