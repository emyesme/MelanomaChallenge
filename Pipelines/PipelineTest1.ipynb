{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10c2bdab",
   "metadata": {},
   "source": [
    "### GENERATING FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6d0f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46964cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import library\n",
    "import color_features\n",
    "import texture_features\n",
    "import glcm_features\n",
    "import hair_removal\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "samples = library.get_sample(path = \"/home/name/Desktop/CAD/challenge1/train\", amount=0)\n",
    "\n",
    "dictF = {}\n",
    "features = pd.DataFrame()\n",
    "count = 0\n",
    "flag = True\n",
    "for sample in samples:\n",
    "    print('count ', count)\n",
    "    count += 1\n",
    "    img = cv2.imread(sample)\n",
    "    output_bh = library.hair_removal_BH(img)\n",
    "    \n",
    "    dictF['name'] = sample\n",
    "    dictF['label'] = (0 if 'nevus' in sample else 1 )\n",
    "    \n",
    "    # color features\n",
    "    colors = color_features.extract_color_features(output_bh)\n",
    "    \n",
    "    dictF.update(colors)\n",
    "    \n",
    "    #glcm features\n",
    "    angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "    distances = [1]\n",
    "    colorspaces = ['rgb', 'hsv', 'lab', 'ycc', 'gray']\n",
    "\n",
    "    for cs in colorspaces:\n",
    "        glcm = glcm_features.get_glcm(output_bh, angles, distances, cs)\n",
    "        dictF.update(glcm)\n",
    "    \n",
    "    # lbp features\n",
    "    lbp = texture_features.extract_lbp(output_bh, 1, 8)\n",
    "    dictF.update(lbp)\n",
    "    \n",
    "    # orb features\n",
    "    # orb = texture_features.extract_orb(output_bh, 64)\n",
    "    # dictF.update(orb)\n",
    "    \n",
    "    features = features.append(dictF, ignore_index=True)\n",
    "    \n",
    "    library.writeFeatures(features,\n",
    "                  flag,\n",
    "                  os.path.join('/home',\n",
    "                             'name',\n",
    "                             'Desktop',\n",
    "                             'CAD'),\n",
    "                  'features_train_bh_3000.csv')\n",
    "    \n",
    "    flag = False\n",
    "    features = pd.DataFrame()\n",
    "    dictF.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf9e210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2630, 423)\n",
      "Searching for best hyperparameters\n",
      "The best parameters for rf are {'criterion': 'gini', 'max_depth': 10, 'n_estimators': 900} with an accuracy of 0.6294\n",
      "### RF ###\n",
      "current pipeline\n",
      "Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('selector rfe',\n",
      "                 RFE(estimator=RandomForestClassifier(n_jobs=-1,\n",
      "                                                      random_state=42))),\n",
      "                ('clf',\n",
      "                 RandomForestClassifier(max_depth=10, n_estimators=900,\n",
      "                                        n_jobs=-1, random_state=42))])\n",
      "###############\n",
      " ### Report ###\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.77      0.78       323\n",
      "           1       0.80      0.81      0.80       355\n",
      "\n",
      "    accuracy                           0.79       678\n",
      "   macro avg       0.79      0.79      0.79       678\n",
      "weighted avg       0.79      0.79      0.79       678\n",
      "\n",
      " ### score ###\n",
      "0.7920353982300885\n",
      " ### accuracy ###\n",
      "0.7920353982300885\n",
      "### f1_score ###\n",
      "0.803347280334728\n",
      "### confusion matrix ###\n",
      "[[249  74]\n",
      " [ 67 288]]\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "### TREE ###\n",
      "current pipeline\n",
      "Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('selector rfe',\n",
      "                 RFE(estimator=RandomForestClassifier(n_jobs=-1,\n",
      "                                                      random_state=42))),\n",
      "                ('clf', DecisionTreeClassifier())])\n",
      "###############\n",
      " ### Report ###\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.72      0.70       323\n",
      "           1       0.73      0.69      0.71       355\n",
      "\n",
      "    accuracy                           0.71       678\n",
      "   macro avg       0.71      0.71      0.70       678\n",
      "weighted avg       0.71      0.71      0.71       678\n",
      "\n",
      " ### score ###\n",
      "0.7050147492625368\n",
      " ### accuracy ###\n",
      "0.7050147492625368\n",
      "### f1_score ###\n",
      "0.7093023255813954\n",
      "### confusion matrix ###\n",
      "[[234  89]\n",
      " [111 244]]\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "The best parameters for svm are {'C': 1, 'gamma': 2.5, 'kernel': 'rbf'} with an accuracy of 0.5412\n",
      "### SVM ###\n",
      "current pipeline\n",
      "Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('selector rfe',\n",
      "                 RFE(estimator=RandomForestClassifier(n_jobs=-1,\n",
      "                                                      random_state=42))),\n",
      "                ('clf',\n",
      "                 SVC(C=1, gamma=2.5, probability=True, random_state=42))])\n",
      "###############\n",
      " ### Report ###\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      1.00      0.65       323\n",
      "           1       0.00      0.00      0.00       355\n",
      "\n",
      "    accuracy                           0.48       678\n",
      "   macro avg       0.24      0.50      0.32       678\n",
      "weighted avg       0.23      0.48      0.31       678\n",
      "\n",
      " ### score ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emily/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/emily/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/emily/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47640117994100295\n",
      " ### accuracy ###\n",
      "0.47640117994100295\n",
      "### f1_score ###\n",
      "0.0\n",
      "### confusion matrix ###\n",
      "[[323   0]\n",
      " [355   0]]\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "The best parameters for ab are {'learning_rate': 1, 'n_estimators': 900} with an accuracy of 0.6471\n",
      "### ADABOOST ###\n",
      "current pipeline\n",
      "Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('selector rfe',\n",
      "                 RFE(estimator=RandomForestClassifier(n_jobs=-1,\n",
      "                                                      random_state=42))),\n",
      "                ('clf',\n",
      "                 AdaBoostClassifier(learning_rate=1, n_estimators=900,\n",
      "                                    random_state=42))])\n",
      "###############\n",
      " ### Report ###\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.77      0.76       323\n",
      "           1       0.79      0.77      0.78       355\n",
      "\n",
      "    accuracy                           0.77       678\n",
      "   macro avg       0.77      0.77      0.77       678\n",
      "weighted avg       0.77      0.77      0.77       678\n",
      "\n",
      " ### score ###\n",
      "0.7713864306784661\n",
      " ### accuracy ###\n",
      "0.7713864306784661\n",
      "### f1_score ###\n",
      "0.7788873038516405\n",
      "### confusion matrix ###\n",
      "[[250  73]\n",
      " [ 82 273]]\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "The best parameters for gb are {'learning_rate': 0.2, 'max_depth': 8, 'n_estimators': 300} with an accuracy of 0.6647\n",
      "### GRADBOOST ###\n",
      "current pipeline\n",
      "Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('selector rfe',\n",
      "                 RFE(estimator=RandomForestClassifier(n_jobs=-1,\n",
      "                                                      random_state=42))),\n",
      "                ('clf',\n",
      "                 GradientBoostingClassifier(learning_rate=0.2, max_depth=8,\n",
      "                                            n_estimators=300,\n",
      "                                            random_state=42))])\n",
      "###############\n",
      " ### Report ###\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.80      0.79       323\n",
      "           1       0.81      0.80      0.80       355\n",
      "\n",
      "    accuracy                           0.80       678\n",
      "   macro avg       0.80      0.80      0.80       678\n",
      "weighted avg       0.80      0.80      0.80       678\n",
      "\n",
      " ### score ###\n",
      "0.7964601769911505\n",
      " ### accuracy ###\n",
      "0.7964601769911505\n",
      "### f1_score ###\n",
      "0.8039772727272727\n",
      "### confusion matrix ###\n",
      "[[257  66]\n",
      " [ 72 283]]\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "### HISTGRADBOOST ###\n",
      "current pipeline\n",
      "Pipeline(steps=[('scale', StandardScaler()),\n",
      "                ('selector rfe',\n",
      "                 RFE(estimator=RandomForestClassifier(n_jobs=-1,\n",
      "                                                      random_state=42))),\n",
      "                ('clf', HistGradientBoostingClassifier())])\n",
      "###############\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import library\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2, mutual_info_classif    \n",
    "\n",
    "#\n",
    "classifiers = [\"rf\", \"tree\", \"svm\", \"adaboost\", \"gradboost\", \"histgradboost\", \"knn\", \"lda\"]\n",
    "\n",
    "#train = pd.read_csv(os.path.join('/home','emily','Desktop','CAD','features_train_bh_3000.csv'))\n",
    "#test = pd.read_csv(os.path.join('/home','emily','Desktop','CAD','features_test_bh_3000.csv'))\n",
    "\n",
    "train1 = pd.read_csv(os.path.join('/home','emily','Desktop','CAD','MelanomaChallenge','features','featuresCh1E_0.csv'))\n",
    "train2 = pd.read_csv(os.path.join('/home','emily','Desktop','CAD','MelanomaChallenge','features','featuresCh1E_1.csv'))\n",
    "train3 = pd.read_csv(os.path.join('/home','emily','Desktop','CAD','MelanomaChallenge','features','featuresCh1E_2.csv'))\n",
    "test = pd.read_csv(os.path.join('/home','emily','Desktop','CAD','MelanomaChallenge','features','featuresCh1E_3.csv'))\n",
    "\n",
    "train = pd.concat([train1, train2, train3])\n",
    "print(train.shape)\n",
    "\n",
    "y = train['label']\n",
    "X = train.drop(['label'], axis=1)\n",
    "X = X.drop(['name'], axis=1)\n",
    "\n",
    "\n",
    "y_test = test['label']\n",
    "X_test = test.drop(['label'], axis=1)\n",
    "X_test = X_test.drop(['name'], axis=1)\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.2, random_state=1)\n",
    "\n",
    "\n",
    "\n",
    "# preprocessing options\n",
    "#('selectFromModel', SelectFromModel(RandomForestClassifier(random_state=42, n_jobs = -1)))\n",
    "#('selector rfe', RFE(RandomForestClassifier(random_state=42, n_jobs = -1))),\n",
    "#('reduce_dims', PCA(n_components=150)),\n",
    "#('mutual_info_classif, SelectKBest(mutual_info_classif, k=100)),\n",
    "\n",
    "for classifier in classifiers:\n",
    "    \n",
    "    # preprocessing steps\n",
    "    pipe = [('scale', StandardScaler()),\n",
    "            ('selector rfe', RFE(RandomForestClassifier(random_state=42, n_jobs = -1)))\n",
    "           ]\n",
    "\n",
    "    \n",
    "    if classifier == \"svm\":\n",
    "        clf, best_params = library.SVC_linear(X_val, y_val, cv=2)\n",
    "        clf.set_params(**best_params)\n",
    "        print(\"### SVM ###\")\n",
    "    \n",
    "    elif classifier == \"rf\":\n",
    "        clf, best_params = library.RandomForest(X_val, y_val, cv=2)\n",
    "        clf.set_params(**best_params)\n",
    "        print(\"### RF ###\")\n",
    "    \n",
    "    elif classifier == \"tree\":\n",
    "        clf = tree.DecisionTreeClassifier()\n",
    "        print(\"### TREE ###\")\n",
    "    \n",
    "    elif classifier == \"adaboost\":\n",
    "        clf, best_params = library.AdaBoost(X_val, y_val)\n",
    "        clf.set_params(**best_params)\n",
    "        print(\"### ADABOOST ###\")\n",
    "    \n",
    "    elif classifier == \"gradboost\":\n",
    "        clf, best_params = library.GradientBoosting(X_val, y_val)\n",
    "        clf.set_params(**best_params)\n",
    "        print(\"### GRADBOOST ###\")\n",
    "    \n",
    "    elif classifier == \"knn\":\n",
    "        clf, best_params = library.knn(X_val, y_val)\n",
    "        clf.set_params(**best_params)\n",
    "        print(\"### KNN ###\")\n",
    "        \n",
    "    elif classifier == \"histgradboost\":\n",
    "        clf = HistGradientBoostingClassifier()\n",
    "        print(\"### HISTGRADBOOST ###\")\n",
    "        \n",
    "    elif classifier == \"lda\":\n",
    "        clf = LinearDiscriminantAnalysis()\n",
    "        print(\"### LDA ###\")        \n",
    "        \n",
    "    # add classifier \n",
    "    pipe.append(tuple(('clf', clf)))\n",
    "    \n",
    "    steps = Pipeline(pipe)\n",
    "    \n",
    "    # pipeline shape\n",
    "    print(\"current pipeline\")\n",
    "    print(steps)\n",
    "    \n",
    "    \n",
    "    library.fit_report(steps, X, y, X_test, y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbf241b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
