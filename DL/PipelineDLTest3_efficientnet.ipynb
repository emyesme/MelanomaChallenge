{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "u27ZtWK1XAY-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model_resnet50.pt', 'model_12_2.pt']\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "#first put a shortcut in your drive to the image processing folder\n",
    "\n",
    "RESULTS_DIR = os.path.join('/home',\n",
    "                        'emily',\n",
    "                        'Desktop',\n",
    "                        'CAD',\n",
    "                         'results' )\n",
    "\n",
    "\n",
    "DATA_DIR = os.path.join('/home',\n",
    "                        'emily',\n",
    "                        'Desktop',\n",
    "                        'CAD',\n",
    "                        'challenge1')\n",
    "\n",
    "print(os.listdir(RESULTS_DIR))\n",
    "\n",
    "data_file = os.listdir(RESULTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_9rhux8SW0Ts"
   },
   "outputs": [],
   "source": [
    "# import the required packages\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "#import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.utils.data as data\n",
    "import copy\n",
    "\n",
    "from torchmetrics import F1Score\n",
    "from torchmetrics.functional import auroc\n",
    "from torchmetrics.functional import precision_recall\n",
    "from torchmetrics import AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "I0CzC_YGW1Yx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32\n",
    "learning_rate = 0.02\n",
    "epochs = 30\n",
    "momentum = 0.1\n",
    "lr_step_size = 1000   # if < epochs, we are using decaying learning rate\n",
    "lr_gamma = 0.1\n",
    "data_augmentation = True\n",
    "dropout = 0.1\n",
    "activation = nn.LeakyReLU()\n",
    "\n",
    "# make visible only one GPU at the time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # <-- should be the ID of the GPU you want to use\n",
    "\n",
    "# options\n",
    "# device = \"cuda:0\"           # put here \"cuda:0\" if you want to run on GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "monitor_display = True      # whether to display monitored performance plots\n",
    "display_first_n = 0         # how many samples/batches are displayed\n",
    "num_workers = 2             # how many workers (=threads) for fetching data\n",
    "pretrained = False          # whether to test a pretrained model (to be loaded) or train a new one\n",
    "display_errors = True       # whether to display errors (only in pretrained mode)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YMoibauPrDjm"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "# We need to use numpy for rotation transformation\n",
    "# For sigma and mean we nee to do patch wise\n",
    "\n",
    "# https://arxiv.org/pdf/2010.05351v1.pdf\n",
    "class MyRotationTransform:\n",
    "    \"\"\"Rotate by one of the given angles.\"\"\"\n",
    "\n",
    "    def __init__(self, times, mode):\n",
    "        self.times = times\n",
    "        self.mode = mode\n",
    "\n",
    "    def __call__(self, x):\n",
    "        mode = random.choice(self.mode)\n",
    "        if mode == 0:\n",
    "          return np.fliplr(x)\n",
    "        elif mode == 1:\n",
    "          return np.flipud(x)\n",
    "        else:   \n",
    "          times = random.choice(self.times)\n",
    "          return np.rot90(x, times)\n",
    "# -90, 0, 90, and 180 degrees rotation\n",
    "\n",
    "#rotation_transform = MyRotationTransform(angles=[-90, 0, 90, 180])\n",
    "# pytorch transformations for augmentation https://arxiv.org/pdf/2010.05351v1.pdf\n",
    "# DataAugmentation = transforms.RandomApply(\n",
    "#         [ np.RandomHorizontalFlip(), \n",
    "#      transforms.RandomVerticalFlip(),\n",
    "#      #transforms.RandomRotation(90, fill=(0,)),\n",
    "#       MyRotationTransform(times=[1,2,3])] , p=0.5)  # fill=(0,) is a workaround for the torchvision bug tracked at https://github.com/pytorch/vision/issues/1759#issuecomment-575307516\n",
    "\n",
    "class Convert(object):\n",
    "    def __call__(self, img):\n",
    "        return torch.unsqueeze(torch.from_numpy(img.astype('float')), 0).float()\n",
    "\n",
    "transform_train = transforms.Compose(\n",
    "    [MyRotationTransform(times=[1,2,3], mode=[0,1,2,3])]) \n",
    "\n",
    "# should randomly apply a transformation from the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gm8zZBDDYdjy"
   },
   "source": [
    "**Load information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(DATA_DIR, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=8,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pz8hf2H6uAaQ"
   },
   "source": [
    "**Dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GM_jp5acK70y"
   },
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DDuCOfTYaKM"
   },
   "source": [
    "### **Model definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Ti68kRkObivY"
   },
   "outputs": [],
   "source": [
    "#from torchsummary import summary\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# pretrained efficient net b3\n",
    "# https://pytorch.org/vision/main/models/generated/torchvision.models.efficientnet_b3.html#torchvision.models.efficientnet_b3\n",
    "efficientb3 = models.efficientnet_b3(weights=models.EfficientNet_B3_Weights.DEFAULT)\n",
    "\n",
    "# https://stackoverflow.com/questions/68047331/how-to-add-the-last-classification-layer-in-efficienet-pre-trained-model-in-pyto\n",
    "# efficientnet._fc = torch.nn.Linear(in_features = efficientb3._fc.in_features,\n",
    "#                                   out_features = NUM_CLASSES,\n",
    "#                                   bias = TRUE)\n",
    "\n",
    "efficientb3.classifier[1] = torch.nn.Linear( in_features = efficientb3.classifier[1].in_features,\n",
    "                                          out_features = NUM_CLASSES,\n",
    "                                          bias = True)\n",
    "#efficientb3.fc = nn.Linear(2048,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-foaC4XPd9A"
   },
   "source": [
    "**Useful Metrics and Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "uB8A7rDS0EG5"
   },
   "outputs": [],
   "source": [
    "def confusion(prediction, truth):\n",
    "    \"\"\" Returns the confusion matrix for the values in the `prediction` and `truth`\n",
    "    tensors, i.e. the amount of positions where the values of `prediction`\n",
    "    and `truth` are\n",
    "    - 1 and 1 (True Positive)\n",
    "    - 1 and 0 (False Positive)\n",
    "    - 0 and 0 (True Negative)\n",
    "    - 0 and 1 (False Negative)\n",
    "    \"\"\"\n",
    "\n",
    "    confusion_vector = prediction / truth\n",
    "    # Element-wise division of the 2 tensors returns a new tensor which holds a\n",
    "    # unique value for each case:\n",
    "    #   1     where prediction and truth are 1 (True Positive)\n",
    "    #   inf   where prediction is 1 and truth is 0 (False Positive)\n",
    "    #   nan   where prediction and truth are 0 (True Negative)\n",
    "    #   0     where prediction is 0 and truth is 1 (False Negative)\n",
    "\n",
    "    true_positives = torch.sum(confusion_vector == 1).item()\n",
    "    false_positives = torch.sum(confusion_vector == float('inf')).item()\n",
    "    true_negatives = torch.sum(torch.isnan(confusion_vector)).item()\n",
    "    false_negatives = torch.sum(confusion_vector == 0).item()\n",
    "\n",
    "    return true_positives, false_positives, true_negatives, false_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Qd_99B-tUrRV"
   },
   "outputs": [],
   "source": [
    "def save(model, path_to_save: str) -> None:\n",
    "    torch.save(model.state_dict(), path_to_save)\n",
    "\n",
    "def load(model, path_to_model: str):\n",
    "    return model.load_state_dict(torch.load(path_to_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1pUh_a6PqMN"
   },
   "source": [
    "**Train and test function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "yLRT9aFctJm9"
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, scheduler, model_name, num_epochs=30, load_trained=False):\n",
    "    # measure of time\n",
    "    since = time.time()\n",
    "    f1 = F1Score(num_classes=2).to(device)\n",
    "\n",
    "    # load previous state of the model\n",
    "    if load_trained:\n",
    "      # if exist in here\n",
    "      checkpoint = torch.load(RESULTS_DIR + model_name)\n",
    "      model.load_state_dict(checkpoint['model_state_dict'])\n",
    "      optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "      last_epoch = checkpoint['epoch']+1\n",
    "      loss = checkpoint['loss']\n",
    "    # restart\n",
    "    else:\n",
    "      last_epoch=0\n",
    "\n",
    "    #It keeps track of the parameters of the model in certain state\n",
    "    best_model_wts = copy.deepcopy(model.state_dict()) \n",
    "    best_auc = 0.0\n",
    "  \n",
    "    # start epoch\n",
    "    for epoch in range(last_epoch, num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "          \n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "    \n",
    "            f1_history = list()\n",
    "\n",
    "            y_true = list()\n",
    "            y_pred = list()\n",
    "\n",
    "\n",
    "            tp_total = 0\n",
    "            fp_total = 0\n",
    "            tn_total = 0\n",
    "            fn_total = 0 \n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloader[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    preds = torch.argmax(outputs, dim=1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                f1_history.append(f1(preds, labels.data).double().cpu().numpy()) \n",
    "                y_true.append(labels.data.cpu())\n",
    "                sig = nn.Sigmoid()\n",
    "                y_pred.append(sig(outputs)[:,0].detach().cpu())\n",
    "\n",
    "                # confusion matrix computation\n",
    "                tp, fp, tn, fn = confusion(preds, labels.data)\n",
    "                tp_total += tp\n",
    "                fp_total += fp\n",
    "                tn_total += tn\n",
    "                fn_total += fn\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_f1 = np.mean(f1_history)\n",
    "            epoch_auc = auroc(torch.cat(y_pred, dim=0), torch.cat(y_true, dim=0), max_fpr=0.0001).item()\n",
    "\n",
    "            if phase == 'train':\n",
    "              scheduler.step()\n",
    "              torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': epoch_loss,\n",
    "                }, RESULTS_DIR+ model_name)  \n",
    "\n",
    "\n",
    "            print('{} Loss: {:.4f} F1-score: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_f1))\n",
    "            print('AUC: ', epoch_auc)\n",
    "            \n",
    "            # plot\n",
    "            fig, ax1 = plt.subplots(figsize=(12,8), num=1)\n",
    "            ax1.set_xticks(np.arange(0, epochs+1, step=epochs/10.0))\n",
    "            ax1.set_xlabel('Epochs')\n",
    "            ax1.set_ylabel(type(criterion).__name__, color='blue')\n",
    "            ax1.set_ylim(0.0001, 10)\n",
    "            ax1.tick_params(axis='y', labelcolor='blue')\n",
    "            ax1.set_yscale('log')\n",
    "            ax1.plot(ticks, losses, 'b-', linewidth=1.0, aa=True, \n",
    "                label='Training (best at ep. %d)' % ticks[np.argmin(epoch_loss)])\n",
    "            ax1.legend(loc=\"lower left\")\n",
    "            ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "            ax2.set_ylabel('Accuracy %', color='red')\n",
    "            ax2.set_ylim(50, 100)\n",
    "            ax2.set_yticks(np.arange(50, 100, step=10))\n",
    "            ax2.tick_params(axis='y', labelcolor='red')\n",
    "            ax2.plot(ticks, train_accuracies, 'r-', linewidth=1.0, aa=True, \n",
    "                label='Training (%.2f%%, best %.2f%% at ep. %d)' % (epoch_auc, max(train_accuracies), ticks[np.argmax(train_accuracies)]))\n",
    "            ax2.plot(ticks, valid_accuracies, 'r--', linewidth=1.0, aa=True, \n",
    "                label='Validation (%.2f%%, best %.2f%% at ep. %d)' % (accuracy_valid, max(valid_accuracies), ticks[np.argmax(valid_accuracies)]))\n",
    "            ax2.legend(loc=\"lower right\")\n",
    "            plt.xlim(0, epochs+1)\n",
    "            # this works if running from notebooks\n",
    "            if run_from_notebook:\n",
    "                fig.show()\n",
    "                fig.canvas.draw()\n",
    "            # this works if running from console\n",
    "            else:\n",
    "                plt.draw()\n",
    "                plt.pause(0.001)\n",
    "            plt.savefig(experiment_ID + \".png\", dpi=300)\n",
    "            fig.clear()\n",
    "            \n",
    "\n",
    "            # deep copy the model if the performance improve\n",
    "            if phase == 'val' and epoch_auc > best_auc:\n",
    "                best_auc = epoch_auc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            del f1_history, y_true, y_pred\n",
    "\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val auc: {:4f}'.format(best_auc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "OnYAcElltZhk"
   },
   "outputs": [],
   "source": [
    "# define test function\n",
    "# returns predictions\n",
    "def test(dataset, dataloader):\n",
    "\n",
    "    # switch to test mode\n",
    "    net.eval()  \n",
    "\n",
    "    # initialize predictions\n",
    "    predictions = []\n",
    "    reals = [] #torch.zeros(len(dataset), dtype=torch.int64)\n",
    "    sample_counter = 0\n",
    "\n",
    "    # do not accumulate gradients (faster)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # test all batches\n",
    "        for inputs, labels in dataloader:\n",
    "\n",
    "            # get data from dataloader [ignore labels/targets as they are not used in test mode]\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # forward pass\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            # store predictions\n",
    "            outputs_max = torch.argmax(outputs, dim=1)\n",
    "            predictions.append( outputs_max)\n",
    "            reals.append(labels.data)\n",
    "            sample_counter += 1 #We should look for MCC, AUC or F1-score, p-AUC- Ratio of positive negatives 10 - 4, 10-3\n",
    "\n",
    "                #90 degrees rotations 1-100, 1-10\n",
    "\n",
    "                #patch wise output into a set of regions\n",
    "                #Using threshold\n",
    "                #output probability map image processing\n",
    "\n",
    "    return predictions, reals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbWX0kP4Qhrw"
   },
   "source": [
    "**Model initialization and Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Emo3pdAQIPe6"
   },
   "outputs": [],
   "source": [
    "net = efficientb3.to(device) #we need to also send the model to the GPU as well\n",
    "\n",
    "# create loss function\n",
    "criterion = nn.CrossEntropyLoss() #most used for classification purposes\n",
    "\n",
    "# create SGD optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum) #most common optimizer is adam\n",
    "\n",
    "# create learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_step_size, gamma=lr_gamma)\n",
    "\n",
    "# experiment ID\n",
    "experiment_ID = \"%s_%s_%s_bs(%d)lr(%.4f_%d_%.1f)m(%.1f)e(%d)act(%s)xavier(yes)da(%s)do(%.1f)BN\" % (type(net).__name__, type(criterion).__name__, type(optimizer).__name__,\n",
    "                batch_size, learning_rate, lr_step_size, lr_gamma, momentum, epochs, type(activation).__name__, data_augmentation, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "SugDntlJSAgh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "----------\n",
      "train Loss: 0.4870 F1-score: 0.7675\n",
      "AUC:  0.4999749958515167\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ticks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_43251/1485287378.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/model_efficientb3.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_trained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_43251/3846575276.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, criterion, optimizer, scheduler, model_name, num_epochs, load_trained)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_yscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             ax1.plot(ticks, losses, 'b-', linewidth=1.0, aa=True, \n\u001b[0m\u001b[1;32m    110\u001b[0m                 label='Training (best at ep. %d)' % ticks[np.argmin(epoch_loss)])\n\u001b[1;32m    111\u001b[0m             \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"lower left\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ticks' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = train_model(net, dataloaders, criterion, optimizer, scheduler, '/model_efficientb3.pt', num_epochs=epochs, load_trained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rf_vzbUBZA-P"
   },
   "outputs": [],
   "source": [
    "save(model, RESULTS_DIR+'/model_try1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFBKnnwrTgaU"
   },
   "source": [
    "## **Model Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affine==2.3.1\n",
      "appdirs==1.4.4\n",
      "apturl==0.5.2\n",
      "argon2-cffi==21.1.0\n",
      "attrs==21.2.0\n",
      "Babel==2.8.0\n",
      "backcall==0.2.0\n",
      "bcrypt==3.2.0\n",
      "beautifulsoup4==4.10.0\n",
      "beniget==0.4.1\n",
      "bleach==4.1.0\n",
      "blinker==1.4\n",
      "Brlapi==0.8.3\n",
      "Brotli==1.0.9\n",
      "caffeine==2.9.8\n",
      "certifi==2020.6.20\n",
      "chardet==4.0.0\n",
      "click==8.0.3\n",
      "click-plugins==1.1.1\n",
      "cligj==0.7.2\n",
      "colorama==0.4.4\n",
      "command-not-found==0.3\n",
      "cryptography==3.4.8\n",
      "cupshelpers==1.0\n",
      "cycler==0.11.0\n",
      "dbus-python==1.2.18\n",
      "decorator==4.4.2\n",
      "defer==1.0.6\n",
      "defusedxml==0.7.1\n",
      "distro==1.7.0\n",
      "distro-info===1.1build1\n",
      "duplicity==0.8.21\n",
      "entrypoints==0.4\n",
      "ewmh==0.1.6\n",
      "fasteners==0.14.1\n",
      "fonttools==4.29.1\n",
      "fs==2.4.12\n",
      "future==0.18.2\n",
      "gast==0.5.2\n",
      "html5lib==1.1\n",
      "httplib2==0.20.2\n",
      "idna==3.3\n",
      "imageio==2.22.0\n",
      "imbalanced-learn==0.9.1\n",
      "imblearn==0.0\n",
      "importlib-metadata==4.6.4\n",
      "intensity-normalization==2.2.3\n",
      "ipykernel==6.7.0\n",
      "ipython==7.31.1\n",
      "ipython_genutils==0.2.0\n",
      "ipywidgets==6.0.0\n",
      "jedi==0.18.0\n",
      "jeepney==0.7.1\n",
      "Jinja2==3.0.3\n",
      "joblib==1.2.0\n",
      "jsonschema==3.2.0\n",
      "jupyter-client==7.1.2\n",
      "jupyter-core==4.9.1\n",
      "jupyterlab-pygments==0.1.2\n",
      "keyring==23.5.0\n",
      "kiwisolver==1.3.2\n",
      "language-selector==0.1\n",
      "launchpadlib==1.10.16\n",
      "lazr.restfulclient==0.14.4\n",
      "lazr.uri==1.0.6\n",
      "libvirt-python==8.0.0\n",
      "lockfile==0.12.2\n",
      "louis==3.20.0\n",
      "lxml==4.8.0\n",
      "lz4==3.1.3+dfsg\n",
      "macaroonbakery==1.3.1\n",
      "mahotas==1.4.13\n",
      "Mako==1.1.3\n",
      "MarkupSafe==2.0.1\n",
      "matplotlib==3.5.1\n",
      "matplotlib-inline==0.1.3\n",
      "mlxtend==0.21.0\n",
      "monotonic==1.6\n",
      "more-itertools==8.10.0\n",
      "mpmath==0.0.0\n",
      "nb-clean==2.3.0\n",
      "nbclient==0.5.6\n",
      "nbconvert==6.4.0\n",
      "nbformat==5.1.3\n",
      "nest-asyncio==1.5.4\n",
      "netifaces==0.11.0\n",
      "networkx==2.8.6\n",
      "nibabel==4.0.2\n",
      "notebook==6.4.8\n",
      "numpy==1.23.4\n",
      "nvidia-cublas-cu11==11.10.3.66\n",
      "nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "nvidia-cuda-runtime-cu11==11.7.99\n",
      "nvidia-cudnn-cu11==8.5.0.96\n",
      "oauthlib==3.2.0\n",
      "olefile==0.46\n",
      "opencv-contrib-python==4.6.0.66\n",
      "opencv-python==4.6.0.66\n",
      "opencv-python-headless==4.6.0.66\n",
      "packaging==21.3\n",
      "pandas==1.5.0\n",
      "pandocfilters==1.5.0\n",
      "paramiko==2.9.3\n",
      "parso==0.8.1\n",
      "patsy==0.5.3\n",
      "pexpect==4.8.0\n",
      "phasepack==1.5\n",
      "pickleshare==0.7.5\n",
      "Pillow==9.0.1\n",
      "ply==3.11\n",
      "prometheus-client==0.9.0\n",
      "prompt-toolkit==3.0.28\n",
      "protobuf==3.12.4\n",
      "ptyprocess==0.7.0\n",
      "py==1.10.0\n",
      "pycairo==1.20.1\n",
      "pycryptodomex==3.11.0\n",
      "pycups==2.0.1\n",
      "pydicom==2.3.0\n",
      "pyFFTW==0.13.0\n",
      "Pygments==2.11.2\n",
      "PyGObject==3.42.1\n",
      "PyJWT==2.3.0\n",
      "pymacaroons==0.13.0\n",
      "pymedio==0.2.13\n",
      "PyNaCl==1.5.0\n",
      "pyparsing==2.4.7\n",
      "pyRFC3339==1.1\n",
      "pyrsistent==0.18.1\n",
      "python-apt==2.3.0+ubuntu2.1\n",
      "python-dateutil==2.8.1\n",
      "python-debian===0.1.43ubuntu1\n",
      "python-xlib==0.29\n",
      "pythran==0.10.0\n",
      "pytz==2022.1\n",
      "PyWavelets==1.4.1\n",
      "pyxattr==0.7.2\n",
      "pyxdg==0.27\n",
      "PyYAML==5.4.1\n",
      "pyzmq==22.3.0\n",
      "rasterio==1.3.3\n",
      "reportlab==3.6.8\n",
      "requests==2.25.1\n",
      "scikit-fuzzy==0.4.2\n",
      "scikit-image==0.19.3\n",
      "scikit-learn==1.1.2\n",
      "scipy==1.8.0\n",
      "screen-resolution-extra==0.0.0\n",
      "seaborn==0.12.1\n",
      "SecretStorage==3.3.1\n",
      "Send2Trash==1.8.1b0\n",
      "SimpleITK==2.2.0\n",
      "six==1.16.0\n",
      "snuggs==1.4.7\n",
      "soupsieve==2.3.1\n",
      "statsmodels==0.13.2\n",
      "sympy==1.9\n",
      "systemd-python==234\n",
      "terminado==0.13.1\n",
      "testpath==0.5.0\n",
      "threadpoolctl==3.1.0\n",
      "tifffile==2022.8.12\n",
      "torch==1.13.0\n",
      "torchmetrics==0.10.3\n",
      "torchvision==0.14.0\n",
      "tornado==6.1\n",
      "tqdm==4.40.0\n",
      "traitlets==5.1.1\n",
      "typing_extensions==4.4.0\n",
      "ubuntu-advantage-tools==27.11.3\n",
      "ubuntu-drivers-common==0.0.0\n",
      "ufoLib2==0.13.1\n",
      "ufw==0.36.1\n",
      "unattended-upgrades==0.1\n",
      "unicodedata2==14.0.0\n",
      "urllib3==1.26.5\n",
      "usb-creator==0.3.7\n",
      "vboxapi==1.0\n",
      "wadllib==1.3.6\n",
      "wcwidth==0.2.5\n",
      "webencodings==0.5.1\n",
      "widgetsnbextension==2.0.0\n",
      "xdg==5\n",
      "xgboost==1.7.1\n",
      "xkit==0.0.0\n",
      "youtube-dl==2021.12.17\n",
      "zipp==1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze | grep pyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
