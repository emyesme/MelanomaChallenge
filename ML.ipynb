{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bf9e210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best hyperparameters\n",
      "The best parameters for rf are {'criterion': 'gini', 'max_depth': 10, 'n_estimators': 500} with an accuracy of 0.7425\n",
      "### RF ###\n",
      " ### Report ###\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.80      0.78       189\n",
      "           1       0.81      0.77      0.79       211\n",
      "\n",
      "    accuracy                           0.79       400\n",
      "   macro avg       0.78      0.79      0.78       400\n",
      "weighted avg       0.79      0.79      0.79       400\n",
      "\n",
      " ### score ###\n",
      "0.785\n",
      " ### accuracy ###\n",
      "0.785\n",
      "### f1_score ###\n",
      "0.79126213592233\n",
      "### confusion matrix ###\n",
      "[[151  38]\n",
      " [ 48 163]]\n",
      "### TREE ###\n",
      " ### Report ###\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.74      0.73       189\n",
      "           1       0.76      0.74      0.75       211\n",
      "\n",
      "    accuracy                           0.74       400\n",
      "   macro avg       0.74      0.74      0.74       400\n",
      "weighted avg       0.74      0.74      0.74       400\n",
      "\n",
      " ### score ###\n",
      "0.7375\n",
      " ### accuracy ###\n",
      "0.7375\n",
      "### f1_score ###\n",
      "0.7482014388489209\n",
      "### confusion matrix ###\n",
      "[[139  50]\n",
      " [ 55 156]]\n",
      "The best parameters for svm are {'C': 1, 'gamma': 2.5, 'kernel': 'rbf'} with an accuracy of 0.5200\n",
      "### SVM ###\n",
      " ### Report ###\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.99      0.67       189\n",
      "           1       0.96      0.11      0.20       211\n",
      "\n",
      "    accuracy                           0.53       400\n",
      "   macro avg       0.73      0.55      0.44       400\n",
      "weighted avg       0.74      0.53      0.42       400\n",
      "\n",
      " ### score ###\n",
      "0.53\n",
      " ### accuracy ###\n",
      "0.53\n",
      "### f1_score ###\n",
      "0.20338983050847453\n",
      "### confusion matrix ###\n",
      "[[188   1]\n",
      " [187  24]]\n",
      "The best parameters for ab are {'learning_rate': 0.1, 'n_estimators': 600} with an accuracy of 0.7175\n",
      "### ADABOOST ###\n",
      " ### Report ###\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.80      0.77       189\n",
      "           1       0.81      0.74      0.77       211\n",
      "\n",
      "    accuracy                           0.77       400\n",
      "   macro avg       0.77      0.77      0.77       400\n",
      "weighted avg       0.77      0.77      0.77       400\n",
      "\n",
      " ### score ###\n",
      "0.77\n",
      " ### accuracy ###\n",
      "0.77\n",
      "### f1_score ###\n",
      "0.7733990147783252\n",
      "### confusion matrix ###\n",
      "[[151  38]\n",
      " [ 54 157]]\n",
      "The best parameters for gb are {'learning_rate': 0.05, 'max_depth': 8, 'n_estimators': 300} with an accuracy of 0.7000\n",
      "### GRADBOOST ###\n",
      " ### Report ###\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.81      0.79       189\n",
      "           1       0.82      0.78      0.80       211\n",
      "\n",
      "    accuracy                           0.80       400\n",
      "   macro avg       0.80      0.80      0.80       400\n",
      "weighted avg       0.80      0.80      0.80       400\n",
      "\n",
      " ### score ###\n",
      "0.7975\n",
      " ### accuracy ###\n",
      "0.7975\n",
      "### f1_score ###\n",
      "0.802919708029197\n",
      "### confusion matrix ###\n",
      "[[154  35]\n",
      " [ 46 165]]\n",
      "### KNN ###\n",
      " ### Report ###\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.76      0.75       189\n",
      "           1       0.78      0.76      0.77       211\n",
      "\n",
      "    accuracy                           0.76       400\n",
      "   macro avg       0.76      0.76      0.76       400\n",
      "weighted avg       0.76      0.76      0.76       400\n",
      "\n",
      " ### score ###\n",
      "0.7575\n",
      " ### accuracy ###\n",
      "0.7575\n",
      "### f1_score ###\n",
      "0.7673860911270983\n",
      "### confusion matrix ###\n",
      "[[143  46]\n",
      " [ 51 160]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import library\n",
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "import os\n",
    "    \n",
    "\n",
    "classifiers = [\"rf\", \"tree\", \"svm\", \"adaboost\", \"gradboost\", \"knn\"]\n",
    "\n",
    "train = pd.read_csv(os.path.join('/home','emily','Desktop','CAD','features_paola.csv'))\n",
    "\n",
    "y = train['label']\n",
    "X = train.drop(['label'], axis=1)\n",
    "X = X.drop(['name'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "\n",
    "for classifier in classifiers:\n",
    "    \n",
    "    if classifier == \"svm\":\n",
    "        clf, best_params = library.SVC_linear(X_val, y_val, cv=2)\n",
    "        clf.set_params(**best_params) \n",
    "        print(\"### SVM ###\")\n",
    "        library.fit_report(clf, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    elif classifier == \"rf\":\n",
    "        clf, best_params = library.RandomForest(X_val, y_val, cv=2)\n",
    "        clf.set_params(**best_params)\n",
    "        print(\"### RF ###\")\n",
    "        library.fit_report(clf, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    elif classifier == \"tree\":\n",
    "        clf = tree.DecisionTreeClassifier()\n",
    "        print(\"### TREE ###\")\n",
    "        library.fit_report(clf, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    elif classifier == \"adaboost\":\n",
    "        clf, best_params = library.AdaBoost(X_val, y_val)\n",
    "        clf.set_params(**best_params)\n",
    "        print(\"### ADABOOST ###\")\n",
    "        library.fit_report(clf, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    elif classifier == \"gradboost\":\n",
    "        clf, best_params = library.GradientBoosting(X_val, y_val)\n",
    "        clf.set_params(**best_params)\n",
    "        print(\"### GRADBOOST ###\")\n",
    "        library.fit_report(clf, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    elif classifier == \"knn\":\n",
    "        clf, best_params = library.knn(X_val, y_val)\n",
    "        clf.set_params(**best_params)\n",
    "        print(\"### KNN ###\")\n",
    "        library.fit_report(clf, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    #elif classifier == \"ensemble\": \n",
    "        #ensemble\n",
    "        #best_paramsSVM = gridSearchSVM(hyperOptTrainData, hyperOptLabel, cv)\n",
    "        # best_paramsRF = gridSearchRF(hyperOptTrainData, hyperOptLabel, cv) 0.18-0.20\n",
    "        #best_paramsGB = gridSearchGradientBoosting(hyperOptTrainData, hyperOptLabel, cv)\n",
    "        #best_paramsAB = gridSearchAdaBoost(hyperOptTrainData, hyperOptLabel, cv)# 0.16-0.19 lr=0.1\n",
    "        #best_paramsLR = gridSearchLogRegre(hyperOptTrainData, hyperOptLabel, cv)\n",
    "\n",
    "        #pipeSVC = Pipeline([('scaler', StandardScaler()), ('svm', SVC(**best_paramsSVM, probability=True))])# false -> class\n",
    "        # pipeRF  = Pipeline([('scaler', StandardScaler()), ('rf', RandomForestClassifier( **best_paramsRF,  n_jobs = -1, random_state=42))])\n",
    "        #pipeAB  = Pipeline([('scaler', StandardScaler()), ('ab', AdaBoostClassifier( **best_paramsAB, random_state=42))]) \n",
    "        #pipeGB  = Pipeline([('scaler', StandardScaler()), ('gb', GradientBoostingClassifier( **best_paramsGB, random_state=42))])\n",
    "        #pipeLR = Pipeline([('scaler', StandardScaler()), ('lr', LogisticRegression(**best_paramsLR, random_state=42))]) \n",
    "        #pipeNB = Pipeline([('scaler', StandardScaler()), ('nb', GaussianNB())])\n",
    "\n",
    "        #clf = VotingClassifier(estimators=[('svm', pipeSVC),\n",
    "        #                                   ('lr', pipeLR),\n",
    "        #                                   ('ab', pipeAB),\n",
    "        #                                   ('gb', pipeGB),\n",
    "        #                                   ('nb', pipeNB)],\n",
    "        #                                   voting = 'soft')\n",
    "\n",
    "        #fit_report(clf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44ab1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
